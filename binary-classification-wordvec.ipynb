{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this BINARY CLASSIFIER to be used is [bc-dataset.csv].\r\n",
    "There are 2 implementations here:\r\n",
    "1. Bag of words approach.\r\n",
    "2. **Word vectors (can be pre-trained word embeddings).**\r\n",
    "\r\n",
    "The dataset split is 60-40.\r\n",
    "Evaluation metrics to be used in this are:\r\n",
    "1. Precision.\r\n",
    "2. Recall.\r\n",
    "3. F-Measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For GloVe\r\n",
    "root_folder = ''\r\n",
    "data_folder = 'data'\r\n",
    "glove_filename = 'glove.42B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftfy import fix_encoding\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "import csv\r\n",
    "from spellchecker import SpellChecker\r\n",
    "import string\r\n",
    "import nltk as nlp\r\n",
    "from nltk.corpus import stopwords\r\n",
    "\r\n",
    "# nltk.download(\"stopwords\")\r\n",
    "# nltk.download('punkt')\r\n",
    "# nltk.download('wordnet')\r\n",
    "\r\n",
    "stop_words = stopwords.words(\"english\")\r\n",
    "spell = SpellChecker()\r\n",
    "\r\n",
    "def correct_spellings(x, spell = spell):\r\n",
    "    x = x.split()\r\n",
    "    misspelled = spell.unknown(x)\r\n",
    "    result = map(lambda word: spell.correction(word) if word in misspelled else word, x)\r\n",
    "    return \" \".join(result)\r\n",
    "\r\n",
    "def tweet_cleaning(x, correct_spelling=True, remove_emojis=True, remove_stop_words=True):\r\n",
    "    x = x.lower().strip()\r\n",
    "    \r\n",
    "    #remove URLs\r\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\r\n",
    "    x = url.sub(r'',x)\r\n",
    "\r\n",
    "    #remove HTML tags\r\n",
    "    html = re.compile(r'<.*?>')\r\n",
    "    x = html.sub(r'',x)\r\n",
    "\r\n",
    "    #strip punctuation\r\n",
    "    operator = str.maketrans('','',string.punctuation)\r\n",
    "    x = x.translate(operator)\r\n",
    "\r\n",
    "    if correct_spelling:\r\n",
    "        x = correct_spellings(x)\r\n",
    "\r\n",
    "    if remove_emojis:\r\n",
    "        x = x.encode('ascii', 'ignore').decode('utf8').strip()\r\n",
    "    \r\n",
    "    if remove_stop_words:\r\n",
    "        x = ' '.join([word for word in x.split(' ') if word not in stop_words])\r\n",
    "\r\n",
    "def fix_encode(x):\r\n",
    "    return fix_encoding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\r\n",
    "pd.set_option('display.max_colwidth', None)\r\n",
    "\r\n",
    "df = pd.read_csv(\r\n",
    "    \"datasets/bc-dataset.csv\",\r\n",
    "    encoding=\"latin1\",\r\n",
    "    sep=\",\",\r\n",
    "    quoting=csv.QUOTE_ALL\r\n",
    ")\r\n",
    "\r\n",
    "data = pd.concat([df.gender, df.description, df.text], axis=1)\r\n",
    "\r\n",
    "#drop null rows\r\n",
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixes any encoding errors and applies a cleaning function on the text column.\r\n",
    "\r\n",
    "Note: shouldn't I be applying both to both columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixes any broken encodings\r\n",
    "data.description = data.description.apply(lambda x: fix_encode(x))\r\n",
    "\r\n",
    "# apply the cleaning function\r\n",
    "data.text = data.text.apply(tweet_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some fun stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(data.shape))\r\n",
    "\r\n",
    "print(\"Just some stats.\")\r\n",
    "print(\"------\")\r\n",
    "print(data[\"gender\"].describe())\r\n",
    "print(\"------\")\r\n",
    "print(data[\"gender\"].value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_female = data[\"gender\"] == \"female\"\r\n",
    "get_male = data[\"gender\"] == \"male\"\r\n",
    "get_brand = data[\"gender\"] == \"brand\"\r\n",
    "\r\n",
    "female_rows = data[get_female]\r\n",
    "male_rows = data[get_male]\r\n",
    "brand_rows = data[get_brand]\r\n",
    "\r\n",
    "print(\"total female tweets: \",female_rows.description.count())\r\n",
    "print(\"total male tweets:   \",male_rows.description.count())\r\n",
    "print(\"total brand tweets:  \",brand_rows.description.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_rows.gender = 0     # female\r\n",
    "male_rows.gender = 1       # male\r\n",
    "brand_rows.gender = 2      # brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [female_rows, male_rows, brand_rows]\r\n",
    "data = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\r\n",
    "\r\n",
    "#glove_input_file = glove_filename\r\n",
    "word2vec_output_file = glove_filename+'.word2vec'\r\n",
    "glove2word2vec(glove_path, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\r\n",
    "# load the Stanford GloVe model\r\n",
    "word2vec_output_file = glove_filename+'.word2vec'\r\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\r\n",
    "\r\n",
    "#Show a word embedding\r\n",
    "print('King: ',model.get_vector('king'))\r\n",
    "\r\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\r\n",
    "\r\n",
    "print('Most similar word to King + Woman: ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer:\r\n",
    "    def __init__(self, model):\r\n",
    "        print(\"Loading in word vectors...\")\r\n",
    "        self.word_vectors = model\r\n",
    "        print(\"Finished loading in word vectors\")\r\n",
    "\r\n",
    "    def fit(self, data):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def transform(self, data):\r\n",
    "        # determine the dimensionality of vectors\r\n",
    "        v = self.word_vectors.get_vector('king')\r\n",
    "        self.D = v.shape[0]\r\n",
    "\r\n",
    "        X = np.zeros((len(data), self.D))\r\n",
    "        n = 0\r\n",
    "        emptycount = 0\r\n",
    "        for sentence in data:\r\n",
    "        tokens = sentence.split()\r\n",
    "        vecs = []\r\n",
    "        m = 0\r\n",
    "        for word in tokens:\r\n",
    "            try:\r\n",
    "            # throws KeyError if word not found\r\n",
    "            vec = self.word_vectors.get_vector(word)\r\n",
    "            vecs.append(vec)\r\n",
    "            m += 1\r\n",
    "            except KeyError:\r\n",
    "            pass\r\n",
    "        if len(vecs) > 0:\r\n",
    "            vecs = np.array(vecs)\r\n",
    "            X[n] = vecs.mean(axis=0)\r\n",
    "        else:\r\n",
    "            emptycount += 1\r\n",
    "        n += 1\r\n",
    "        print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\r\n",
    "        return X\r\n",
    "\r\n",
    "\r\n",
    "    def fit_transform(self, data):\r\n",
    "        self.fit(data)\r\n",
    "        return self.transform(data)\r\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Word2VecVectorizer(model)\r\n",
    "\r\n",
    "y = data.gender.values\r\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\r\n",
    "\r\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "clf = RandomForestClassifier(n_estimates=200)\r\n",
    "clf.fit(X_train, y_train)\r\n",
    "\r\n",
    "print(\"train score:\", clf.score(Xtrain, Ytrain))\r\n",
    "print(\"test score:\", clf.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "y_pred_rfc = clf.predict(X_test)\r\n",
    "\r\n",
    "print(metrics.classification_report(y_test, y_pred_rfc,  digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "PARAMETERS = {\r\n",
    "    'C':[1.0, 10],\r\n",
    "    'gamma':[1, 'auto', 'scale']\r\n",
    "}\r\n",
    "\r\n",
    "model = GridSearchCV(SVC(kernel='rbf'), PARAMETERS, cv=5, n_jobs=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = model.predict(X_test)\r\n",
    "\r\n",
    "print(metrics.classification_report(y_test, y_pred_svm,  digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "def f1_metric(ytrue,preds):\r\n",
    "    return 'f1_score', f1_score((preds>=0.5).astype('int'), ytrue, average='macro'), True\r\n",
    "\r\n",
    "PARAMETERS_XGB = {\r\n",
    "    'learning_rate': 0.06,\r\n",
    "    'n_estimators': 1500,\r\n",
    "    'colsample_bytree': 0.5,\r\n",
    "    'metric': 'f1_score'\r\n",
    "}\r\n",
    "\r\n",
    "full_clf = LGBMClassifier(**PARAMETERS_XGB)\r\n",
    "\r\n",
    "full_clf.fit(X_train.astype(np.float32), y_train, eval_set=[(X_train.astype(np.float32), y_train), (X_test.astype(np.float32), y_test)],\r\n",
    "                verbose = 400, eval_metric=f1_metric)\r\n",
    "\r\n",
    "print(\"train score:\", full_clf.score(X_train.astype(np.float32), y_train))\r\n",
    "print(\"test score:\", full_clf.score(X_test.astype(np.float32), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = full_clf.predict(X_test.astype(np.float32))\r\n",
    "\r\n",
    "print(metrics.classification_report(y_test, y_pred_xgb,  digits=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python385jvsc74a57bd0f5bd28ee32a81f75b587c4a5c9ba5abfdf032b3105de37173de2ab9e920c5085"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}