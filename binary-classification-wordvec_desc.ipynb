{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this BINARY CLASSIFIER to be used is [bc-dataset.csv].\r\n",
    "There are 2 implementations here:\r\n",
    "1. Bag of words approach.\r\n",
    "2. **Word vectors (can be pre-trained word embeddings).**\r\n",
    "\r\n",
    "The dataset split is 60-40.\r\n",
    "Evaluation metrics to be used in this are:\r\n",
    "1. Precision.\r\n",
    "2. Recall.\r\n",
    "3. F-Measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/pre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from ftfy import fix_encoding\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "import csv\r\n",
    "import string\r\n",
    "import nltk as nlp\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Flatten\r\n",
    "from keras.layers import Embedding\r\n",
    "\r\n",
    "nlp.download(\"stopwords\")\r\n",
    "nlp.download('punkt')\r\n",
    "nlp.download('wordnet')\r\n",
    "\r\n",
    "#For GloVe\r\n",
    "GLOVE_FILENAME = 'glove.42B.300d.txt'\r\n",
    "STOP_WORDS = stopwords.words(\"english\")\r\n",
    "\r\n",
    "def remove_URL(text):\r\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\r\n",
    "    return url.sub(r'',text)\r\n",
    "\r\n",
    "def remove_html(text):\r\n",
    "    html=re.compile(r'<.*?>')\r\n",
    "    return html.sub(r'',text)\r\n",
    "\r\n",
    "def remove_emoji(text):\r\n",
    "    emoji_pattern = re.compile(\"[\"\r\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
    "                           u\"\\U00002702-\\U000027B0\"\r\n",
    "                           u\"\\U000024C2-\\U0001F251\"\r\n",
    "                           \"]+\", flags=re.UNICODE)\r\n",
    "    return emoji_pattern.sub(r'', text)\r\n",
    "\r\n",
    "def remove_punct(text):\r\n",
    "    table=str.maketrans('','',string.punctuation)\r\n",
    "    return text.translate(table)\r\n",
    "\r\n",
    "def fix_encode(x):\r\n",
    "    return fix_encoding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (20050, 3)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\r\n",
    "pd.set_option('display.max_colwidth', None)\r\n",
    "\r\n",
    "df = pd.read_csv(\r\n",
    "    \"datasets/bc-dataset.csv\",\r\n",
    "    encoding=\"latin1\",\r\n",
    "    sep=\",\",\r\n",
    "    quoting=csv.QUOTE_ALL\r\n",
    ")\r\n",
    "\r\n",
    "data = pd.concat([df.gender, df['gender:confidence'], df.description], axis=1)\r\n",
    "\r\n",
    "#drop null rows\r\n",
    "print(\"Data Shape: \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(data.shape))\r\n",
    "\r\n",
    "data.dropna(subset=['description', 'gender'], inplace=True)\r\n",
    "data = data.reset_index(drop=True)\r\n",
    "\r\n",
    "data.drop(data[data['gender'] == 'brand'].index, inplace=True)\r\n",
    "# print(str(data.shape))\r\n",
    "\r\n",
    "data.drop(data[data['gender:confidence'] < 0.80].index, inplace=True)\r\n",
    "data.drop('gender:confidence', axis=1, inplace=True)\r\n",
    "\r\n",
    "data.description = data.description.apply(lambda x: fix_encode(x))\r\n",
    "# print(str(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['description']=data['description'].apply(lambda x : remove_URL(x))\r\n",
    "data['description']=data['description'].apply(lambda x : remove_html(x))\r\n",
    "data['description']=data['description'].apply(lambda x : remove_emoji(x))\r\n",
    "data['description']=data['description'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:  Support West Ham live in NZ eat macarons  support West Ham If there was a D it would be silent Proud husband Father to a gorgeous girl May 28 2015\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL: \", data.description.iloc[213])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nlp.WordNetLemmatizer()\r\n",
    "description_list = []\r\n",
    "\r\n",
    "for each in data.description:\r\n",
    "    each = re.sub(\"[^a-zA-Z]\",\" \", str(each))                                        # regex to clean unnecesarry chars\r\n",
    "    each = each.lower()                                                              # lowercase all\r\n",
    "    each = nlp.word_tokenize(each)                                                   # split all by tokenizing\r\n",
    "    each = [word for word in each if not word in set(stopwords.words(\"english\"))]    # delete stop words from your array\r\n",
    "    each = [lemma.lemmatize(word) for word in each]                                  # lemmatize \"memories\" -> \"memory\"\r\n",
    "    each = \" \".join(each)                                                            # make them one string again\r\n",
    "    # each = correct_spellings(each)                                                   # correct the spelling (?)\r\n",
    "    description_list.append(each)                                                         # put them into big array\r\n",
    "\r\n",
    "# data['description'] = data['description'].apply(tweet_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:  Support West Ham live in NZ eat macarons  support West Ham If there was a D it would be silent Proud husband Father to a gorgeous girl May 28 2015\n",
      "PREPROCESSED:  support west ham live nz eat macarons support west ham would silent proud husband father gorgeous girl may\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL: \", data.description.iloc[213])\r\n",
    "print(\"PREPROCESSED: \", description_list[213])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total female tweets:  4620\n",
      "total male tweets:    4155\n"
     ]
    }
   ],
   "source": [
    "get_female = data[\"gender\"] == \"female\"\r\n",
    "get_male = data[\"gender\"] == \"male\"\r\n",
    "# get_brand = data[\"gender\"] == \"brand\"\r\n",
    "\r\n",
    "female_rows = data[get_female]\r\n",
    "male_rows = data[get_male]\r\n",
    "# brand_rows = data[get_brand]\r\n",
    "\r\n",
    "print(\"total female tweets: \", female_rows.description.count())\r\n",
    "print(\"total male tweets:   \", male_rows.description.count())\r\n",
    "# print(\"total brand tweets:  \", brand_rows.description.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py:5170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "female_rows.gender = 0     # female\r\n",
    "male_rows.gender = 1       # male\r\n",
    "# brand_rows.gender = 2      # brand) \r\n",
    "\r\n",
    "frames = [female_rows, male_rows]\r\n",
    "data = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "y = data.gender.values\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\r\n",
    "from tensorflow.python.keras.preprocessing import text\r\n",
    "\r\n",
    "TOP_K = 20000 #number of features\r\n",
    "MAX_SEQ_LEN = 50 #length of text sequences\r\n",
    "\r\n",
    "class BC_Tokenizer():\r\n",
    "    def __init__(self, train_text):\r\n",
    "        self.train_text = train_text\r\n",
    "        self.tokenizer = Tokenizer(num_words=TOP_K)\r\n",
    "    \r\n",
    "    def train_tokenize(self):\r\n",
    "        max_len = len(max(self.train_text, key=len))\r\n",
    "        self.max_len = min(max_len, MAX_SEQ_LEN)\r\n",
    "\r\n",
    "        # vocabulary\r\n",
    "        self.tokenizer.fit_on_texts(self.train_text)\r\n",
    "    \r\n",
    "    def vectorize_input(self, texts):\r\n",
    "        #vectorize training\r\n",
    "        texts = self.tokenizer.texts_to_sequences(texts)\r\n",
    "        texts = sequence.pad_sequences(texts, maxlen=self.max_len, truncating='post', padding='post')\r\n",
    "        return texts\r\n",
    "    \r\n",
    "tokenizer = BC_Tokenizer(train_text=X_train.description)\r\n",
    "\r\n",
    "#fit\r\n",
    "tokenizer.train_tokenize()\r\n",
    "tokenized_train = tokenizer.vectorize_input(X_train['description'])\r\n",
    "tokenized_test = tokenizer.vectorize_input(X_test['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15508/15508 [00:00<00:00, 353405.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\r\n",
    "\r\n",
    "EMBEDDING_VECTOR_LENGTH = 50 #<=200\r\n",
    "\r\n",
    "def construct_embedding_matrix(glove_file, word_index):\r\n",
    "    embedding_dict={}\r\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\r\n",
    "        for line in f:\r\n",
    "            values=line.split()\r\n",
    "            # get the word\r\n",
    "            word=values[0]\r\n",
    "            if word in word_index.keys():\r\n",
    "                # get the vector\r\n",
    "                vector = np.asarray(values[1:], 'float32')\r\n",
    "                embedding_dict[word] = vector #OOVs mapped to 0 vectors\r\n",
    "    \r\n",
    "    num_words = len(word_index)+1\r\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_VECTOR_LENGTH))\r\n",
    "\r\n",
    "    for word, i in tqdm.tqdm(word_index.items()):\r\n",
    "        if i < num_words:\r\n",
    "            vect=embedding_dict.get(word, [])\r\n",
    "            if len(vect)>0:\r\n",
    "                embedding_matrix[i] = vect[:EMBEDDING_VECTOR_LENGTH]\r\n",
    "    return embedding_matrix\r\n",
    "\r\n",
    "embedding_matrix = construct_embedding_matrix(GLOVE_FILENAME, tokenizer.tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\r\n",
    "\r\n",
    "def recall_m(y_true, y_pred):\r\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
    "    return recall\r\n",
    "\r\n",
    "def precision_m(y_true, y_pred):\r\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\r\n",
    "    return precision\r\n",
    "\r\n",
    "def f1_m(y_true, y_pred):\r\n",
    "    precision = precision_m(y_true, y_pred)\r\n",
    "    recall = recall_m(y_true, y_pred)\r\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15509\n",
      "15509\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape[0])\r\n",
    "print(len(tokenizer.tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "99/99 - 8s - loss: 0.6919 - acc: 0.5239 - f1_m: 0.0280 - precision_m: 0.0694 - recall_m: 0.0236 - val_loss: 0.6903 - val_acc: 0.5218 - val_f1_m: 0.0184 - val_precision_m: 0.1439 - val_recall_m: 0.0099\n",
      "Epoch 2/10\n",
      "99/99 - 5s - loss: 0.6791 - acc: 0.5815 - f1_m: 0.3986 - precision_m: 0.5732 - recall_m: 0.3403 - val_loss: 0.6670 - val_acc: 0.5983 - val_f1_m: 0.4193 - val_precision_m: 0.6736 - val_recall_m: 0.3136\n",
      "Epoch 3/10\n",
      "99/99 - 5s - loss: 0.6659 - acc: 0.6056 - f1_m: 0.4606 - precision_m: 0.6470 - recall_m: 0.3869 - val_loss: 0.6614 - val_acc: 0.6078 - val_f1_m: 0.5018 - val_precision_m: 0.6417 - val_recall_m: 0.4233\n",
      "Epoch 4/10\n",
      "99/99 - 5s - loss: 0.6565 - acc: 0.6280 - f1_m: 0.5355 - precision_m: 0.6551 - recall_m: 0.4766 - val_loss: 0.6594 - val_acc: 0.6049 - val_f1_m: 0.5201 - val_precision_m: 0.6234 - val_recall_m: 0.4596\n",
      "Epoch 5/10\n",
      "99/99 - 5s - loss: 0.6554 - acc: 0.6249 - f1_m: 0.5645 - precision_m: 0.6279 - recall_m: 0.5419 - val_loss: 0.6541 - val_acc: 0.6197 - val_f1_m: 0.6012 - val_precision_m: 0.5980 - val_recall_m: 0.6176\n",
      "Epoch 6/10\n",
      "99/99 - 5s - loss: 0.6502 - acc: 0.6360 - f1_m: 0.5857 - precision_m: 0.6478 - recall_m: 0.5707 - val_loss: 0.6935 - val_acc: 0.6078 - val_f1_m: 0.4101 - val_precision_m: 0.7360 - val_recall_m: 0.2937\n",
      "Epoch 7/10\n",
      "99/99 - 5s - loss: 0.6436 - acc: 0.6413 - f1_m: 0.5707 - precision_m: 0.6517 - recall_m: 0.5296 - val_loss: 0.6479 - val_acc: 0.6244 - val_f1_m: 0.5511 - val_precision_m: 0.6370 - val_recall_m: 0.4967\n",
      "Epoch 8/10\n",
      "99/99 - 5s - loss: 0.6455 - acc: 0.6436 - f1_m: 0.5644 - precision_m: 0.6747 - recall_m: 0.5143 - val_loss: 0.6523 - val_acc: 0.6254 - val_f1_m: 0.6347 - val_precision_m: 0.5941 - val_recall_m: 0.6964\n",
      "Epoch 9/10\n",
      "99/99 - 5s - loss: 0.6405 - acc: 0.6379 - f1_m: 0.5622 - precision_m: 0.6564 - recall_m: 0.5155 - val_loss: 0.6525 - val_acc: 0.6182 - val_f1_m: 0.6118 - val_precision_m: 0.5905 - val_recall_m: 0.6480\n",
      "Epoch 10/10\n",
      "99/99 - 5s - loss: 0.6375 - acc: 0.6293 - f1_m: 0.5252 - precision_m: 0.6574 - recall_m: 0.4719 - val_loss: 0.6375 - val_acc: 0.6339 - val_f1_m: 0.5536 - val_precision_m: 0.6635 - val_recall_m: 0.4854\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding,LSTM,Dense,Dropout\r\n",
    "from keras.initializers import Constant\r\n",
    "from keras.optimizers import Adam\r\n",
    "\r\n",
    "model = Sequential()\r\n",
    "\r\n",
    "embedding = Embedding(len(tokenizer.tokenizer.word_index)+1,#unique tokens\r\n",
    "                        EMBEDDING_VECTOR_LENGTH, #no. of features\r\n",
    "                        embeddings_initializer=Constant(embedding_matrix), #initialize\r\n",
    "                        input_length=MAX_SEQ_LEN,\r\n",
    "                        trainable=False\r\n",
    "                        )\r\n",
    "\r\n",
    "model.add(embedding)\r\n",
    "model.add(Dropout(0.2))\r\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.5))\r\n",
    "model.add(Dense(1, activation='sigmoid'))\r\n",
    "\r\n",
    "#compile\r\n",
    "optimizer = Adam(clipvalue=0.5) #clip value to prevent the gradient exploding (?)\r\n",
    "\r\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc', f1_m, precision_m,recall_m])\r\n",
    "\r\n",
    "history = model.fit(tokenized_train, y_train, \r\n",
    "                    batch_size = 32,\r\n",
    "                    epochs=10,\r\n",
    "                    #validation_data=(tokenized_test,y_test),\r\n",
    "                    validation_split=0.4,\r\n",
    "                    verbose=2)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, f1_score, precision, recall = model.evaluate(tokenized_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  0.6322489380836487\n",
      "Accuracy :  0.6452991366386414\n",
      "F1 Score :  0.5362495183944702\n",
      "Precision :  0.6806806921958923\n",
      "Recall :  0.45623141527175903\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss : \", loss)\r\n",
    "print(\"Accuracy : \", accuracy)\r\n",
    "print(\"F1 Score : \", f1_score)\r\n",
    "print(\"Precision : \", precision)\r\n",
    "print(\"Recall : \", recall)\r\n",
    "\r\n",
    "# print(f'F1 Score: {\"#.5f\"%f1_score}')\r\n",
    "# print(f'Precision: {\"#.5f\"%precision}')\r\n",
    "# print(f'Recall: {\"#.5f\"%recall}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python385jvsc74a57bd0f5bd28ee32a81f75b587c4a5c9ba5abfdf032b3105de37173de2ab9e920c5085"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}