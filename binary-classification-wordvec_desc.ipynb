{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this BINARY CLASSIFIER to be used is [bc-dataset.csv].\r\n",
    "There are 2 implementations here:\r\n",
    "1. Bag of words approach.\r\n",
    "2. **Word vectors (can be pre-trained word embeddings).**\r\n",
    "\r\n",
    "The dataset split is 60-40.\r\n",
    "Evaluation metrics to be used in this are:\r\n",
    "1. Precision.\r\n",
    "2. Recall.\r\n",
    "3. F-Measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/pre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from ftfy import fix_encoding\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "import csv\r\n",
    "import string\r\n",
    "import nltk as nlp\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Flatten\r\n",
    "from keras.layers import Embedding\r\n",
    "\r\n",
    "nlp.download(\"stopwords\")\r\n",
    "nlp.download('punkt')\r\n",
    "nlp.download('wordnet')\r\n",
    "\r\n",
    "#For GloVe\r\n",
    "GLOVE_FILENAME = 'glove.42B.300d.txt'\r\n",
    "STOP_WORDS = stopwords.words(\"english\")\r\n",
    "\r\n",
    "def remove_URL(text):\r\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\r\n",
    "    return url.sub(r'',text)\r\n",
    "\r\n",
    "def remove_html(text):\r\n",
    "    html=re.compile(r'<.*?>')\r\n",
    "    return html.sub(r'',text)\r\n",
    "\r\n",
    "def remove_emoji(text):\r\n",
    "    emoji_pattern = re.compile(\"[\"\r\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
    "                           u\"\\U00002702-\\U000027B0\"\r\n",
    "                           u\"\\U000024C2-\\U0001F251\"\r\n",
    "                           \"]+\", flags=re.UNICODE)\r\n",
    "    return emoji_pattern.sub(r'', text)\r\n",
    "\r\n",
    "def remove_punct(text):\r\n",
    "    table=str.maketrans('','',string.punctuation)\r\n",
    "    return text.translate(table)\r\n",
    "\r\n",
    "def fix_encode(x):\r\n",
    "    return fix_encoding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (20050, 3)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\r\n",
    "pd.set_option('display.max_colwidth', None)\r\n",
    "\r\n",
    "df = pd.read_csv(\r\n",
    "    \"datasets/bc-dataset.csv\",\r\n",
    "    encoding=\"latin1\",\r\n",
    "    sep=\",\",\r\n",
    "    quoting=csv.QUOTE_ALL\r\n",
    ")\r\n",
    "\r\n",
    "data = pd.concat([df.gender, df['gender:confidence'], df.description], axis=1)\r\n",
    "\r\n",
    "#drop null rows\r\n",
    "print(\"Data Shape: \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(data.shape))\r\n",
    "\r\n",
    "data.dropna(subset=['description', 'gender'], inplace=True)\r\n",
    "data = data.reset_index(drop=True)\r\n",
    "\r\n",
    "# print(str(data.shape))\r\n",
    "\r\n",
    "data.drop(data[data['gender:confidence'] < 0.80].index, inplace=True)\r\n",
    "data.drop('gender:confidence', axis=1, inplace=True)\r\n",
    "\r\n",
    "data.description = data.description.apply(lambda x: fix_encode(x))\r\n",
    "# print(str(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['description']=data['description'].apply(lambda x : remove_URL(x))\r\n",
    "data['description']=data['description'].apply(lambda x : remove_html(x))\r\n",
    "data['description']=data['description'].apply(lambda x : remove_emoji(x))\r\n",
    "data['description']=data['description'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:  waiting for Gandalf to come to my house and invite me on an adventure from which I will not expect to return\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL: \", data.description.iloc[213])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nlp.WordNetLemmatizer()\r\n",
    "description_list = []\r\n",
    "\r\n",
    "for each in data.description:\r\n",
    "    each = re.sub(\"[^a-zA-Z]\",\" \", str(each))                                        # regex to clean unnecesarry chars\r\n",
    "    each = each.lower()                                                              # lowercase all\r\n",
    "    each = nlp.word_tokenize(each)                                                   # split all by tokenizing\r\n",
    "    each = [word for word in each if not word in set(stopwords.words(\"english\"))]    # delete stop words from your array\r\n",
    "    each = [lemma.lemmatize(word) for word in each]                                  # lemmatize \"memories\" -> \"memory\"\r\n",
    "    each = \" \".join(each)                                                            # make them one string again\r\n",
    "    # each = correct_spellings(each)                                                   # correct the spelling (?)\r\n",
    "    description_list.append(each)                                                         # put them into big array\r\n",
    "\r\n",
    "# data['description'] = data['description'].apply(tweet_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:  waiting for Gandalf to come to my house and invite me on an adventure from which I will not expect to return\n",
      "PREPROCESSED:  waiting gandalf come house invite adventure expect return\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL: \", data.description.iloc[213])\r\n",
    "print(\"PREPROCESSED: \", description_list[213])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total female tweets:  4620\n",
      "total male tweets:    4155\n",
      "total brand tweets:   3011\n"
     ]
    }
   ],
   "source": [
    "get_female = data[\"gender\"] == \"female\"\r\n",
    "get_male = data[\"gender\"] == \"male\"\r\n",
    "get_brand = data[\"gender\"] == \"brand\"\r\n",
    "\r\n",
    "female_rows = data[get_female]\r\n",
    "male_rows = data[get_male]\r\n",
    "brand_rows = data[get_brand]\r\n",
    "\r\n",
    "print(\"total female tweets: \", female_rows.description.count())\r\n",
    "print(\"total male tweets:   \", male_rows.description.count())\r\n",
    "print(\"total brand tweets:  \", brand_rows.description.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py:5170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "female_rows.gender = 0     # female\r\n",
    "male_rows.gender = 1       # male\r\n",
    "brand_rows.gender = 2      # brand) \r\n",
    "\r\n",
    "frames = [female_rows, male_rows, brand_rows]\r\n",
    "data = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "y = data.gender.values\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\r\n",
    "from tensorflow.python.keras.preprocessing import text\r\n",
    "\r\n",
    "TOP_K = 20000 #number of features\r\n",
    "MAX_SEQ_LEN = 50 #length of text sequences\r\n",
    "\r\n",
    "class BC_Tokenizer():\r\n",
    "    def __init__(self, train_text):\r\n",
    "        self.train_text = train_text\r\n",
    "        self.tokenizer = Tokenizer(num_words=TOP_K)\r\n",
    "    \r\n",
    "    def train_tokenize(self):\r\n",
    "        max_len = len(max(self.train_text, key=len))\r\n",
    "        self.max_len = min(max_len, MAX_SEQ_LEN)\r\n",
    "\r\n",
    "        # vocabulary\r\n",
    "        self.tokenizer.fit_on_texts(self.train_text)\r\n",
    "    \r\n",
    "    def vectorize_input(self, texts):\r\n",
    "        #vectorize training\r\n",
    "        texts = self.tokenizer.texts_to_sequences(texts)\r\n",
    "        texts = sequence.pad_sequences(texts, maxlen=self.max_len, truncating='post', padding='post')\r\n",
    "        return texts\r\n",
    "    \r\n",
    "tokenizer = BC_Tokenizer(train_text=X_train.description)\r\n",
    "\r\n",
    "#fit\r\n",
    "tokenizer.train_tokenize()\r\n",
    "tokenized_train = tokenizer.vectorize_input(X_train['description'])\r\n",
    "tokenized_test = tokenizer.vectorize_input(X_test['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19491/19491 [00:00<00:00, 207886.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\r\n",
    "\r\n",
    "EMBEDDING_VECTOR_LENGTH = 50 #<=200\r\n",
    "\r\n",
    "def construct_embedding_matrix(glove_file, word_index):\r\n",
    "    embedding_dict={}\r\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\r\n",
    "        for line in f:\r\n",
    "            values=line.split()\r\n",
    "            # get the word\r\n",
    "            word=values[0]\r\n",
    "            if word in word_index.keys():\r\n",
    "                # get the vector\r\n",
    "                vector = np.asarray(values[1:], 'float32')\r\n",
    "                embedding_dict[word] = vector #OOVs mapped to 0 vectors\r\n",
    "    \r\n",
    "    num_words = len(word_index)+1\r\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_VECTOR_LENGTH))\r\n",
    "\r\n",
    "    for word, i in tqdm.tqdm(word_index.items()):\r\n",
    "        if i < num_words:\r\n",
    "            vect=embedding_dict.get(word, [])\r\n",
    "            if len(vect)>0:\r\n",
    "                embedding_matrix[i] = vect[:EMBEDDING_VECTOR_LENGTH]\r\n",
    "    return embedding_matrix\r\n",
    "\r\n",
    "embedding_matrix = construct_embedding_matrix(GLOVE_FILENAME, tokenizer.tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\r\n",
    "\r\n",
    "def recall_m(y_true, y_pred):\r\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
    "    return recall\r\n",
    "\r\n",
    "def precision_m(y_true, y_pred):\r\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\r\n",
    "    return precision\r\n",
    "\r\n",
    "def f1_m(y_true, y_pred):\r\n",
    "    precision = precision_m(y_true, y_pred)\r\n",
    "    recall = recall_m(y_true, y_pred)\r\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19492\n",
      "19440\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape[0])\r\n",
    "print(len(tokenizer.tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "221/221 - 30s - loss: 0.3047 - acc: 0.3479 - f1_m: 0.7493 - precision_m: 0.6133 - recall_m: 0.9965 - val_loss: -1.8030e-02 - val_acc: 0.3591 - val_f1_m: 0.7578 - val_precision_m: 0.6143 - val_recall_m: 1.0000\n",
      "Epoch 2/20\n",
      "221/221 - 16s - loss: -2.3550e-01 - acc: 0.3478 - f1_m: 0.7487 - precision_m: 0.6040 - recall_m: 0.9965 - val_loss: -1.2194e+00 - val_acc: 0.3591 - val_f1_m: 0.7578 - val_precision_m: 0.6143 - val_recall_m: 1.0000\n",
      "Epoch 3/20\n",
      "221/221 - 17s - loss: -9.8919e-01 - acc: 0.3490 - f1_m: 0.7501 - precision_m: 0.6046 - recall_m: 0.9985 - val_loss: -2.0500e+00 - val_acc: 0.3591 - val_f1_m: 0.7578 - val_precision_m: 0.6143 - val_recall_m: 1.0000\n",
      "Epoch 4/20\n",
      "221/221 - 18s - loss: -1.6633e+00 - acc: 0.3534 - f1_m: 0.7517 - precision_m: 0.6168 - recall_m: 0.9797 - val_loss: -2.9738e+00 - val_acc: 0.3644 - val_f1_m: 0.7603 - val_precision_m: 0.6273 - val_recall_m: 0.9763\n",
      "Epoch 5/20\n",
      "221/221 - 15s - loss: -2.4219e+00 - acc: 0.3473 - f1_m: 0.7481 - precision_m: 0.6049 - recall_m: 0.9933 - val_loss: -3.8826e+00 - val_acc: 0.3612 - val_f1_m: 0.7606 - val_precision_m: 0.6249 - val_recall_m: 0.9834\n",
      "Epoch 6/20\n",
      "221/221 - 16s - loss: -3.4872e+00 - acc: 0.3489 - f1_m: 0.7494 - precision_m: 0.6066 - recall_m: 0.9914 - val_loss: -5.0142e+00 - val_acc: 0.3591 - val_f1_m: 0.7578 - val_precision_m: 0.6143 - val_recall_m: 1.0000\n",
      "Epoch 7/20\n",
      "221/221 - 14s - loss: -3.9557e+00 - acc: 0.3509 - f1_m: 0.7505 - precision_m: 0.6086 - recall_m: 0.9894 - val_loss: -3.9405e+00 - val_acc: 0.3591 - val_f1_m: 0.7578 - val_precision_m: 0.6143 - val_recall_m: 1.0000\n",
      "Epoch 8/20\n",
      "221/221 - 16s - loss: -4.4249e+00 - acc: 0.3520 - f1_m: 0.7507 - precision_m: 0.6123 - recall_m: 0.9831 - val_loss: -5.6146e+00 - val_acc: 0.3591 - val_f1_m: 0.7578 - val_precision_m: 0.6143 - val_recall_m: 1.0000\n",
      "Epoch 9/20\n",
      "221/221 - 16s - loss: -4.9501e+00 - acc: 0.3497 - f1_m: 0.7491 - precision_m: 0.6077 - recall_m: 0.9898 - val_loss: -6.7439e+00 - val_acc: 0.3593 - val_f1_m: 0.7579 - val_precision_m: 0.6145 - val_recall_m: 1.0000\n",
      "Epoch 10/20\n",
      "221/221 - 13s - loss: -5.7745e+00 - acc: 0.3500 - f1_m: 0.7489 - precision_m: 0.6138 - recall_m: 0.9758 - val_loss: -8.6085e+00 - val_acc: 0.3593 - val_f1_m: 0.7579 - val_precision_m: 0.6145 - val_recall_m: 1.0000\n",
      "Epoch 11/20\n",
      "221/221 - 13s - loss: -7.7199e+00 - acc: 0.3506 - f1_m: 0.7465 - precision_m: 0.6136 - recall_m: 0.9662 - val_loss: -9.7977e+00 - val_acc: 0.3597 - val_f1_m: 0.7581 - val_precision_m: 0.6149 - val_recall_m: 0.9997\n",
      "Epoch 12/20\n",
      "221/221 - 13s - loss: -7.7782e+00 - acc: 0.3520 - f1_m: 0.7476 - precision_m: 0.6166 - recall_m: 0.9617 - val_loss: -1.0092e+01 - val_acc: 0.3593 - val_f1_m: 0.7579 - val_precision_m: 0.6147 - val_recall_m: 0.9997\n",
      "Epoch 13/20\n",
      "221/221 - 13s - loss: -8.8910e+00 - acc: 0.3571 - f1_m: 0.7490 - precision_m: 0.6218 - recall_m: 0.9556 - val_loss: -1.1144e+01 - val_acc: 0.3595 - val_f1_m: 0.7580 - val_precision_m: 0.6146 - val_recall_m: 1.0000\n",
      "Epoch 14/20\n",
      "221/221 - 13s - loss: -9.8247e+00 - acc: 0.3517 - f1_m: 0.7469 - precision_m: 0.6197 - recall_m: 0.9551 - val_loss: -1.2793e+01 - val_acc: 0.3586 - val_f1_m: 0.7575 - val_precision_m: 0.6142 - val_recall_m: 0.9993\n",
      "Epoch 15/20\n",
      "221/221 - 13s - loss: -1.0114e+01 - acc: 0.3579 - f1_m: 0.7482 - precision_m: 0.6287 - recall_m: 0.9380 - val_loss: -1.3666e+01 - val_acc: 0.3582 - val_f1_m: 0.7568 - val_precision_m: 0.6171 - val_recall_m: 0.9895\n",
      "Epoch 16/20\n",
      "221/221 - 13s - loss: -1.1296e+01 - acc: 0.3596 - f1_m: 0.7491 - precision_m: 0.6319 - recall_m: 0.9352 - val_loss: -1.2426e+01 - val_acc: 0.3593 - val_f1_m: 0.7580 - val_precision_m: 0.6146 - val_recall_m: 1.0000\n",
      "Epoch 17/20\n",
      "221/221 - 13s - loss: -1.2507e+01 - acc: 0.3619 - f1_m: 0.7434 - precision_m: 0.6362 - recall_m: 0.9100 - val_loss: -1.5651e+01 - val_acc: 0.3578 - val_f1_m: 0.7565 - val_precision_m: 0.6140 - val_recall_m: 0.9965\n",
      "Epoch 18/20\n",
      "221/221 - 13s - loss: -1.4680e+01 - acc: 0.3623 - f1_m: 0.7427 - precision_m: 0.6413 - recall_m: 0.8993 - val_loss: -1.7307e+01 - val_acc: 0.4098 - val_f1_m: 0.7426 - val_precision_m: 0.8471 - val_recall_m: 0.6740\n",
      "Epoch 19/20\n",
      "221/221 - 13s - loss: -1.4386e+01 - acc: 0.3656 - f1_m: 0.7445 - precision_m: 0.6426 - recall_m: 0.9019 - val_loss: -1.4047e+01 - val_acc: 0.4104 - val_f1_m: 0.7546 - val_precision_m: 0.7501 - val_recall_m: 0.7705\n",
      "Epoch 20/20\n",
      "221/221 - 13s - loss: -1.6147e+01 - acc: 0.3681 - f1_m: 0.7451 - precision_m: 0.6484 - recall_m: 0.8915 - val_loss: -1.8988e+01 - val_acc: 0.3584 - val_f1_m: 0.7572 - val_precision_m: 0.6141 - val_recall_m: 0.9986\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding,LSTM,Dense,Dropout\r\n",
    "from keras.initializers import Constant\r\n",
    "from keras.optimizers import Adam\r\n",
    "\r\n",
    "model = Sequential()\r\n",
    "\r\n",
    "embedding = Embedding(len(tokenizer.tokenizer.word_index)+53,#unique tokens\r\n",
    "                        EMBEDDING_VECTOR_LENGTH, #no. of features\r\n",
    "                        embeddings_initializer=Constant(embedding_matrix), #initialize\r\n",
    "                        input_length=MAX_SEQ_LEN,\r\n",
    "                        trainable=False\r\n",
    "                        )\r\n",
    "\r\n",
    "model.add(embedding)\r\n",
    "model.add(Dropout(0.2))\r\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.5))\r\n",
    "model.add(Dense(1, activation='sigmoid'))\r\n",
    "\r\n",
    "#compile\r\n",
    "optimizer = Adam(clipvalue=0.5) #clip value to prevent the gradient exploding (?)\r\n",
    "\r\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc', f1_m, precision_m,recall_m])\r\n",
    "\r\n",
    "history = model.fit(tokenized_train, y_train, \r\n",
    "                    batch_size = 32,\r\n",
    "                    epochs=20,\r\n",
    "                    validation_data=(tokenized_test,y_test),\r\n",
    "                    verbose=2)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, f1_score, precision, recall = model.evaluate(tokenized_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score :  0.7572255730628967\n",
      "Precision :  0.6140822172164917\n",
      "Recall :  0.9986042380332947\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score : \", f1_score)\r\n",
    "print(\"Precision : \", precision)\r\n",
    "print(\"Recall : \", recall)\r\n",
    "\r\n",
    "# print(f'F1 Score: {\"#.5f\"%f1_score}')\r\n",
    "# print(f'Precision: {\"#.5f\"%precision}')\r\n",
    "# print(f'Recall: {\"#.5f\"%recall}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python385jvsc74a57bd0f5bd28ee32a81f75b587c4a5c9ba5abfdf032b3105de37173de2ab9e920c5085"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}